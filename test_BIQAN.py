import argparse
'''Usage
python test_BIQAN.py  --dataset_image_path dataset/CC_Resume_All32 \
                        --pre_trained_model checkpoint/PolyU_Mulit_UN_GN/mobilenet/weights.004-0.046.hdf5 \
                        --output_path result/CC_Resume_All32 \
                        --start_index 90 --gpu_id 3

'''
parser = argparse.ArgumentParser(description="Training Blind Image Quality Assessment Network")
parser.add_argument('-id', '--script-id', type=str, default='BlindDenoising', 
                    help="ID of this experiment")

parser.add_argument("--start_index", type=int, default=40,
                    help='index of start number')
parser.add_argument("--dataset_image_path", type=str, default='dataset/CC_Multi_Uniform_GN_F_Origin',
                    help="path of image generated by deep image prior")   

parser.add_argument("--image_size", type=int, default=224,
                    help='size of image')
parser.add_argument("--class_num", type=int, default=10,
                    help='number of final dense layer')
parser.add_argument("--classification_model", type=str, default='mobilenet',
                    help="model used for classification")    
parser.add_argument("--pre_trained_model", type=str, default='checkpoint/PolyU_Mulit_UN_GN/mobilenet/weights.005-0.055.hdf5',  
                    help="Pre-trained classification model")

parser.add_argument("--output_path", type=str, default='result/CC_Multi_Uniform_GN_F_Origin',
                    help="output dir")

parser.add_argument("--gpu_id", type=int, default=0,
                    help="the id of gpu card")

args = parser.parse_args()

print('Start up Script')
print("\n================= Training Blind Image Quality Assessment Network =================")
print("> Parameters:")
for p, v in zip(args.__dict__.keys(), args.__dict__.values()):
    print('\t{}: {}'.format(p, v))

import os
os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)
 
import numpy as np
import pandas as pd
import cv2
import glob
from PIL import Image
import argparse
import time
from PIL import Image
from path import Path

import tensorflow as tf
from keras import backend as K
from keras.models import Model
from keras.layers import Dense, Dropout
from keras.applications.mobilenet import MobileNet
from keras.preprocessing.image import load_img, img_to_array
from keras.applications.mobilenet import preprocess_input

from common_utils import *
from classification_model import get_classication_model


# Call BIQAN to predict the quality of image file 
def get_score(image_files, model, target_size=(224,224)):
    # Return the filename of best predicted reconstructed image and corresponding index and all result
 
    result_list = []
    for image_file in image_files:
        img = load_img(image_file, target_size=target_size)
        x = img_to_array(img)
        x = np.expand_dims(x, axis=0)

        x = preprocess_input(x)

        scores = model.predict(x, batch_size=1, verbose=0)[0]

        mean = mean_score(scores)
        std = std_score(scores)

        result_list.append((mean, std))
         
    result_list = np.array(result_list)
    max_score_index = np.argmax(result_list[:,0])
    return image_files[max_score_index], max_score_index, np.array(result_list)

# Load model trained on reconstructed images
t = time.time()
model = get_classication_model(model_name=args.classification_model, image_size=args.image_size, class_num=args.class_num)
model.load_weights(args.pre_trained_model)
print('Load model cost: %s s'%(time.time() - t))

# Variable
best_denoised_psnrs = []
best_denoisied_gt_psnrs = []
cc_smooth_psnrs = []
os.makedirs(args.output_path, exist_ok=True)

dataset_image_files = glob.glob(args.dataset_image_path + '/*.txt')
image_ids = []
for txt_file in dataset_image_files:
    # Vaild file like dataset/CC_Multi_Uniform_GN_F_Origin/d800_iso3200_1_real.txt
    if (txt_file.split('_')[-1]!='real.txt'):
        continue

    # Output image_folder
    image_folder = txt_file.split('.txt')[0]
    image_id = '_'.join(image_folder.split('/')[-1].split('_')[:-1])
    image_ids.append(image_id)
    print('\nProcessing image: %s...'%image_id)
    output_save_path = os.path.join(args.output_path, image_id)

    # Load reconstructed images 
    reconstructed_files = glob.glob(image_folder + '/*.jpg')

    # Sort according to the filename
    index =  [int(str(img).split('.')[0].split('/')[-1]) for img in reconstructed_files]
    start_index = args.start_index
    sorted_reconstructed_files = [x for _,x in sorted(zip(index, reconstructed_files))][start_index:]
    
    # Load ground truth PSNR
    data = pd.read_csv(txt_file, sep = ' ', names = ['files','psnr', 'ssim']) 
    psnr_gts_sm = data['psnr'].tolist()[args.start_index:]  
    best_gt_psnr, best_gt_index = get_max_value(psnr_gts_sm)
    best_desnoised_gt_img = Image.open(sorted_reconstructed_files[best_gt_index])
    best_denoisied_gt_psnrs.append(best_gt_psnr)
    plot_list(y=psnr_gts_sm, title='PSNR of Ground truth', label='PSNR', filename=output_save_path+'_gt_PNSR', color='r')
    print('\tGround truth: file:%s, psnr:%.2fdB'%(sorted_reconstructed_files[best_gt_index], best_gt_psnr))

    # IQA by BIQAN model
    best_denoised_img_file, best_denoised_psnr_index, denoised_result = get_score(sorted_reconstructed_files, model) 
    best_denoised_psnr = psnr_gts_sm[best_denoised_psnr_index]
    best_denoised_psnrs.append(best_denoised_psnr)
    best_denoised_img = Image.open(best_denoised_img_file)
    cv2.imwrite(output_save_path + '_BIQAN.png', np.array(best_denoised_img)[:,:,::-1])
    plot_list(y=denoised_result[:,0], title='Mean Score of BIQAN', label='Mean Score', filename=output_save_path+'_BIQAN_Mean_Score', color='g')
    plot_list(y=denoised_result[:,1], title='Std Score of BIQAN', label='Std Score', filename=output_save_path+'_BIQAN_Std_Score', color='b')
    print('\tDenoised by BIQAN: file:%s, psnr:%.2fdB'%(best_denoised_img_file, best_denoised_psnr))

    # Low-pass filtering the score
    filter_mean_score = denoised_result[0,0]
    filter_std_score = denoised_result[0,1]
    smooth_denoised_result = denoised_result.copy()
    for index,(mean, std) in enumerate(denoised_result):
        filter_mean_score = float(filter_mean_score) * 0.8 + float(mean) * 0.2
        filter_std_score = float(filter_std_score) * 0.8 + float(std) * 0.2
        smooth_denoised_result[index] = [filter_mean_score, filter_std_score]
    plot_list(y=smooth_denoised_result[:,0], title='Smooth Mean Score of BIQAN', label='Mean Score', filename=output_save_path+'_BIQAN_Mean_Score_Smooth', color='g')
    plot_list(y=smooth_denoised_result[:,1], title='Smooth  Std Score of BIQAN', label='Std Score', filename=output_save_path+'_BIQAN_Std_Score_Smooth', color='b')   

    # Merge the image and show
    merge = np.hstack((np.array(best_desnoised_gt_img),np.array(best_denoised_img)))
    save_image(filename=output_save_path+"_Compare", img=merge, to_RGB=True)

# Print and  save information
info_file = open(args.output_path+"/Denoised_PSNR.txt", 'w')
print('\n========Summary========')
for index,(filename, gt_psnr, denoised_psnr) in enumerate(sorted(zip(image_ids, best_denoisied_gt_psnrs, best_denoised_psnrs))):
    info = '%d: %s, gt PSNR: %.3fdB, denoised PSNR: %.2fdB, loss: %.2fdB\n'%(index+1, filename, float(gt_psnr), float(denoised_psnr), float(gt_psnr)-float(denoised_psnr))
    info_file.write(info)
    print(info)
info = 'Mean of best denoised gt psnr: %.3fdB\n'%mean_psnr(best_denoisied_gt_psnrs)
info_file.write(info)
print(info)
info = 'Mean of best denoised psnrs: %.3fdB\n'%mean_psnr(best_denoised_psnrs)
info_file.write(info)
print(info)
info_file.close()

# Clear session
K.clear_session()