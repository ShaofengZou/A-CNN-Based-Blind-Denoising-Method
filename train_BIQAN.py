import argparse
'''Usage
python train_BIQAN.py   --dataset_image_path dataset/PolyU_Mulit_UN_GN\
                        --dataset_file_path dataset/PolyU_Mulit_UN_GN_PNSR.txt\
                        --output_checkpoint checkpoint/PolyU_Mulit_UN_GN/mobilenet\
                        --output_path result/PolyU_Mulit_UN_GN\
                        --epochs 40 --lr 1e-3 --gpu_id 4
'''
parser = argparse.ArgumentParser(description="Training Blind Image Quality Assessment Network")
parser.add_argument('-id', '--script-id', type=str, default='BlindDenoising', 
                    help="ID of this experiment")

parser.add_argument("--image_size", type=int, default=224,
                    help='size of image')
parser.add_argument("--class_num", type=int, default=10,
                    help='number of final dense layer')
parser.add_argument("--batchsize", type=int, default=32,
                    help='batchsize')
parser.add_argument("--epochs", type=int, default=20,
                    help='number of time the learning of each training stage')
parser.add_argument("--lr", type=float, default=1e-4,
                    help='learning rate of training')
parser.add_argument("--classification_model", type=str, default='mobilenet',
                    help="model used for classification")    
parser.add_argument("--dataset_image_path", type=str, default='dataset/PolyU_Mulit_UN_GN',
                    help="path of image generated by deep image prior")        
parser.add_argument("--dataset_file_path", type=str, default='dataset/PolyU_Mulit_UN_GN_PNSR.txt',  
                    help="path of image file generated by deep image prior")

parser.add_argument("--output_checkpoint", type=str, default='checkpoint',
                    help="checkpoint dir")
parser.add_argument("--output_path", type=str, default='result',
                    help="output dir")
parser.add_argument("--gpu_id", type=int, default=0,
                    help="the id of gpu card")

args = parser.parse_args()

print('Start up Script')
print("\n================= Training Blind Image Quality Assessment Network =================")
print("> Parameters:")
for p, v in zip(args.__dict__.keys(), args.__dict__.values()):
    print('\t{}: {}'.format(p, v))


import os
os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)

import numpy as np
import os
import glob
import cv2
from sklearn.model_selection import train_test_split
import warnings

import tensorflow as tf
import keras
from keras.callbacks import ModelCheckpoint, TensorBoard
from keras.optimizers import Adam, SGD
from keras import backend as K 
from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau

from classification_model import get_classication_model, earth_mover_loss
from dataset_util import read_dataset, data_generator
from common_utils import plot_history
warnings.filterwarnings("ignore")

# Parameters
image_size = args.image_size
class_num = args.class_num
batchsize = args.batchsize
lr = args.lr
epochs = args.epochs
IMAGE_SIZE = (image_size,image_size)

# Madedir
os.makedirs(args.output_checkpoint, exist_ok=True)
os.makedirs(args.output_path, exist_ok=True)

# Load Dataset
# files = glob.glob(args.dataset_image_path + "/*/*.jpg")
# files = sorted(files)
print("Loading datasset...")
file_paths, scores = read_dataset(args.dataset_file_path)
train_files, val_files, train_scores, val_scores = \
    train_test_split(file_paths, scores, test_size=0.2, shuffle=False)
print('Size of training set: ', train_files.shape, train_scores.shape)
print('Size of validation set: ', val_files.shape, val_scores.shape)

# Callback
checkpoint = ModelCheckpoint(args.output_checkpoint + "/weights.{epoch:03d}-{val_loss:.3f}.hdf5",
                                monitor="val_loss",
                                verbose=1,
                                mode="min",
                                save_best_only=True)
earlystop = EarlyStopping(patience=8, verbose=1)
learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=2, min_lr=0.000001, verbose=1)
callbacks = [earlystop, learning_rate_reduction, checkpoint]
 
# Load model and compile
model = get_classication_model(model_name=args.classification_model, image_size=image_size, class_num=class_num)
model.summary()
model.compile(optimizer=Adam(lr=lr), loss=earth_mover_loss)

# Start training
train_generator =  data_generator(file_paths=train_files, scores=train_scores, batchsize=batchsize, image_size=IMAGE_SIZE)
test_generator = data_generator(file_paths=val_files, scores=val_scores,batchsize=batchsize, image_size=IMAGE_SIZE)
print('============First Stage: Training==============')
history = model.fit_generator(generator=train_generator,
                    steps_per_epoch=(len(train_files) // batchsize),
                    epochs=args.epochs, verbose=1, callbacks=callbacks,
                    validation_data=test_generator,
                    validation_steps=(len(val_files) // batchsize))

# Plot training
plot_history(history, args.output_path + '/first stage')                                                                            


# Fine tune
print('============Second Stage: Fine-Tuning==============')
for layer in model.layers:
    layer.trainable = True

model.compile(optimizer=SGD(lr=args.lr, momentum=0.9), loss=earth_mover_loss)

history = model.fit_generator(generator=train_generator,
                    steps_per_epoch=(len(train_files) // batchsize),
                    epochs=args.epochs, verbose=1, callbacks=callbacks,
                    validation_data=test_generator,
                    validation_steps=(len(val_files) // batchsize))

# Plot training
plot_history(history, args.output_path + '/second stage') 